{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](figures/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Requisitos previos\n",
    "\n",
    "Para aprovechar al máximo este lab, ya debería disponer de los siguientes conocimientos:\n",
    "\n",
    "- Declarar variables, escribir bucles y usar sentencias if / else en C.\n",
    "- Definir e invocar funciones en C.\n",
    "- Asignar matrices en C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sistemas heterogéneos\n",
    "\n",
    "Los *sistemas heterogéneos*, son aquellos compuestos por CPU y GPU. En estos sistemas que comunmente también se denomina acelerador a la GPU necesitan de un control por parte del host o CPU que, a su vez, lanzan funciones que se beneficiarán del paralelismo masivo proporcionado por las GPU. La información sobre la GPU se puede consultar con el comando de línea de comando `nvidia-smi` (*Systems Management Interface*). Ejecute el comando `nvidia-smi` ahora, mediante `CTRL` + `ENTER` en la celda de ejecución de código a continuación. Encontrará estas celdas a lo largo de este laboratorio cada vez que necesite ejecutar código. El resultado de ejecutar el comando se imprimirá justo debajo de la celda de ejecución del código después de que se ejecute el código. Después de ejecutar el bloque de ejecución de código inmediatamente debajo, busque y anote el nombre de la GPU en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Escritura de código para la GPU\n",
    "\n",
    "CUDA proporciona extensiones para muchos lenguajes de programación comunes, en el caso de este laboratorio se utilizará el estandard C/C++. Estas extensiones permiten a los desarrolladores ejecutar fácilmente funciones en su código fuente en una GPU.\n",
    "\n",
    "A continuación se muestra un archivo `.cu` (`.cu` es la extensión de archivo para los programas acelerados por CUDA). Contiene dos funciones, la primera que se ejecutará en la CPU, la segunda que se ejecutará en la GPU. Dedique un poco de tiempo a identificar las diferencias entre las funciones, tanto en términos de cómo se definen como de cómo se invocan.\n",
    "\n",
    "\n",
    "```cpp\n",
    "void CPUFunction()\n",
    "{\n",
    "  printf(\"This function is defined to run on the CPU.\\n\");\n",
    "}\n",
    "\n",
    "__global__ void GPUFunction()\n",
    "{\n",
    "  printf(\"This function is defined to run on the GPU.\\n\");\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  CPUFunction();\n",
    "\n",
    "  GPUFunction<<<1, 1>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "}\n",
    "```\n",
    "\n",
    "Aquí hay algunas líneas de código a destacar, así como algunos otros términos comunes que se usan en computación sobre GPUs:\n",
    "\n",
    "`__global__ void GPUFunction()`\n",
    "  - La palabra clave `__global__` indica que la siguiente función se ejecutará en la GPU y se puede invocar **globalmente**, lo que en este contexto significa su ejecución se ordena desde la CPU, es decir el **lanzamiento del kernel**.\n",
    "  - A menudo, el código que se ejecuta en la CPU se denomina código de **host** y el código que se ejecuta en la GPU se denomina código de **device** o **dispositivo**.\n",
    "  - Preste atención en el tipo de devolución `void`. Se requiere que las funciones definidas con la palabra clave `__global__` devuelvan el tipo `void`.\n",
    "\n",
    "`FunciónGPU<<<1, 1>>>();`\n",
    "  - Por lo general, cuando llamamos a una función para que se ejecute en la GPU, llamamos a esta función **kernel**.\n",
    "  - Al iniciar un kernel, debemos proporcionar una **configuración de ejecución**, que se realiza mediante el uso de la sintaxis `<<< ... >>>` justo antes de pasar al kernel los argumentos esperados.\n",
    "  - La configuración de ejecución permite a los programadores especificar la **jerarquía de subprocesos** para el lanzamiento de un kernel, que define la cantidad de agrupaciones de subprocesos (llamados **CUDA bloques**), así como cuántos **subprocesos** o **CUDA threads** a ejecutar en cada bloque. La configuración de ejecución se explorará extensamente más adelante en el laboratorio, pero por el momento, tenga en cuenta que el kernel se inicia con un bloque de subprocesos \"1\" (el primer argumento de configuración de ejecución) que contiene un subproceso \"1\" (el segundo argumento de configuración) .\n",
    "\n",
    "`cudaDeviceSynchronize();`\n",
    "  - A diferencia de gran parte del código C/C++, el lanzamiento de kernels es **asincrónico**: el código de la CPU continuará ejecutándose *sin esperar a que se complete el lanzamiento del kernel*.\n",
    "  - La llamada a `cudaDeviceSynchronize`, hará que el código del host (CPU) espere hasta que se complete el código del dispositivo (GPU), y solo entonces reanudará la ejecución en la CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compilación y ejecución de código CUDA\n",
    "\n",
    "Esta sección contiene detalles sobre el comando `nvcc` que permite compilar y ejecutar su programa `.cu`.\n",
    "\n",
    "La GPU de NVIDIA se puede usar mediante el [**NVIDIA CUDA Compiler**](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html) `nvcc`, que puede compilar aplicaciones en CUDA, tanto el host como el código del dispositivo. Más información del compilador `nvcc` se puede encontrar en [la documentación] (http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html).\n",
    "\n",
    "El uso de `nvcc` será muy parecido si has utilizado previamente un compilador como `gcc`. Compilar, por ejemplo, un archivo `some-CUDA.cu`, es simplemente escribir en la consola:\n",
    "\n",
    "`nvcc -arch=sm_70 -o out some-CUDA.cu -run`\n",
    "  - `nvcc` es el comando de línea de comando para usar el compilador `nvcc`.\n",
    "  - Se pasa `some-CUDA.cu` como archivo a compilar.\n",
    "  - El indicador `o` se usa para especificar el archivo de salida para el programa compilado.\n",
    "  - El indicador `arch` indica para qué **arquitectura** se deben compilar los archivos. Para el caso presente, `sm_70` servirá para compilar específicamente para la GPU en la que se ejecuta este laboratorio, pero para aquellos interesados ​​un conocimiento más profundo, se puede consulte los documentos sobre el flag [`arch`] (http://docs. nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-steering-gpu-code-generation), [características de arquitectura virtual](http://docs.nvidia.com/cuda/cuda -compiler-driver-nvcc/index.html#gpu-feature-list) y [funciones de GPU](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature -lista).\n",
    "  - Como una cuestión de conveniencia, proporcionar el indicador `run` ejecutará el binario compilado con éxito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Jerarquía de bloques e hilos \n",
    "\n",
    "## Lanzamiento de kernels paralelos\n",
    "\n",
    "La configuración de ejecución permite a los programadores especificar detalles sobre el lanzamiento del kernel para que se ejecute en paralelo en múltiples GPU **subprocesos**. Más en concreto, la configuración de ejecución permite a los programadores especificar cuántos grupos de subprocesos, llamados **bloques de subprocesos**, o simplemente **bloques CUDA**, y cuántos subprocesos les gustaría que contuviera cada bloque de subprocesos. La sintaxis para esto es:\n",
    "\n",
    "`<<< NÚMERO_DE_BLOQUES, NÚMERO_DE_HILOS_POR_BLOQUE>>>`\n",
    "\n",
    "** El código del kernel es ejecutado por cada subproceso en cada bloque de subprocesos configurado cuando se inicia el kernel**.\n",
    "\n",
    "Por lo tanto, bajo el supuesto de que se ha definido un kernel llamado `someKernel`, lo siguiente es cierto:\n",
    "  - `someKernel<<<1, 1>>>()` está configurado para ejecutarse en un bloque de un solo subproceso que tiene un solo subproceso y, por lo tanto, se ejecutará solo una vez.\n",
    "  - `someKernel<<<1, 10>>>()` está configurado para ejecutarse en un solo bloque de subprocesos que tiene 10 subprocesos y, por lo tanto, se ejecutará 10 veces.\n",
    "  - `someKernel<<<10, 1>>>()` está configurado para ejecutarse en 10 bloques de subprocesos, cada uno de los cuales tiene un solo subproceso y, por lo tanto, se ejecutará 10 veces.\n",
    "  - `someKernel<<<10, 10>>>()` está configurado para ejecutarse en 10 bloques de subprocesos, cada uno de los cuales tiene 10 subprocesos y, por lo tanto, se ejecutará 100 veces.\n",
    "\n",
    "![Jerarquía de hilos y bloques CUDA](figures/cuda_blocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Indices para hilos y bloques CUDA\n",
    "\n",
    "A cada subproceso se le asigna un índice dentro de su bloque de subprocesos, que comienza en `0`. Además, a cada bloque se le asigna un índice, que comienza en `0`. Así como los subprocesos se agrupan en bloques de subprocesos, los bloques se agrupan en un **grid**, que es la entidad más alta en la jerarquía de subprocesos de CUDA. En resumen, los kernels CUDA se ejecutan en un grid de 1 o más bloques, y cada bloque contiene la misma cantidad de 1 o más subprocesos (hilos).\n",
    "\n",
    "Los kernels CUDA tienen acceso a variables especiales que identifican tanto el índice del subproceso (dentro del bloque) que ejecuta el núcleo como el índice del bloque (dentro de la cuadrícula) en el que se encuentra el subproceso. Estas variables son `threadIdx.x` y `blockIdx.x` respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Paralelizando bucles\n",
    "\n",
    "Los bucles for en las aplicaciones de CPU son objetivos para la aceleración: en lugar de ejecutar cada iteración del bucle en serie, cada iteración del bucle se puede ejecutar en paralelo. Considere el siguiente for y observe, aunque es obvio, la variable *i* controla cuántas veces se ejecutará el bucle:\n",
    "\n",
    "```cpp\n",
    "int N = 2<<20;\n",
    "for (int i = 0; i < N; ++i)\n",
    "{\n",
    "  printf(\"%d\\n\", i);\n",
    "}\n",
    "```\n",
    "\n",
    "Para paralelizar este bucle, se deben seguir 2 pasos:\n",
    "\n",
    "- Se debe escribir un kernel para hacer el trabajo de una **única iteración del bucle**.\n",
    "- Debido a que el kernel será independiente de otros kernels en ejecución, la configuración de ejecución debe ser tal que el kernel se ejecute la cantidad correcta de veces, por ejemplo, la cantidad de veces que se habría iterado el bucle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asignación de memoria para acceder a la GPU y la CPU\n",
    "\n",
    "Las versiones más recientes de CUDA (versión 6 y posteriores) han facilitado la asignación de memoria que está disponible tanto para el host de la CPU como para cualquier cantidad de dispositivos de GPU, y aunque existen muchas [técnicas intermedias y avanzadas] (http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations) para la gestión de memoria que reportará un rendimiento más óptimo, la gestión de memoria CUDA más cómoda para desarrolladores se basa en la idea de memoria unificada.\n",
    "\n",
    "Para asignar y liberar memoria, y obtener un puntero al que se pueda hacer referencia tanto en el código del host como del dispositivo, reemplace las llamadas a `malloc` y `free` con `cudaMallocManaged` y `cudaFree` como en el siguiente ejemplo:\n",
    "\n",
    "\n",
    "```cpp\n",
    "// CPU-only\n",
    "\n",
    "int N = 2<<20;\n",
    "size_t size = N * sizeof(int);\n",
    "\n",
    "int *a;\n",
    "a = (int *)malloc(size);\n",
    "\n",
    "// Use `a` in CPU-only program.\n",
    "\n",
    "free(a);\n",
    "```\n",
    "\n",
    "```cpp\n",
    "// Accelerated\n",
    "\n",
    "int N = 2<<20;\n",
    "size_t size = N * sizeof(int);\n",
    "\n",
    "int *a;\n",
    "// Note the address of `a` is passed as first argument.\n",
    "cudaMallocManaged(&a, size);\n",
    "\n",
    "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
    "\n",
    "cudaFree(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manejo de Errores\n",
    "\n",
    "Como en cualquier aplicación, el manejo de errores en código CUDA acelerado es fundamental. Muchas, si no la mayoría de las funciones de CUDA (consulte, por ejemplo, las [funciones de administración de memoria](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY)) devuelven un valor de tipo `cudaError_t`, que se puede utilizar para comprobar si se ha producido o no un error al llamar a la función. Aquí hay un ejemplo donde se realiza el manejo de errores para una llamada a `cudaMallocManaged`:\n",
    "\n",
    "```cpp\n",
    "cudaError_t err;\n",
    "err = cudaMallocManaged(&a, N)                    // Assume the existence of `a` and `N`.\n",
    "\n",
    "if (err != cudaSuccess)                           // `cudaSuccess` is provided by CUDA.\n",
    "{\n",
    "  printf(\"Error: %s\\n\", cudaGetErrorString(err)); // `cudaGetErrorString` is provided by CUDA.\n",
    "}\n",
    "```\n",
    "\n",
    "Los kernels que están definidos para devolver `void`, no devuelven un valor de tipo `cudaError_t`. Para comprobar si se producen errores en el momento del lanzamiento del kernel, por ejemplo, si la configuración de lanzamiento es errónea, CUDA proporciona la función `cudaGetLastError`, que devuelve un valor de tipo `cudaError_t`.\n",
    "\n",
    "```cpp\n",
    "/*\n",
    " * This launch should cause an error, but the kernel itself\n",
    " * cannot return it.\n",
    " */\n",
    "\n",
    "someKernel<<<1, -1>>>();  // -1 is not a valid number of threads.\n",
    "\n",
    "cudaError_t err;\n",
    "err = cudaGetLastError(); // `cudaGetLastError` will return the error from above.\n",
    "if (err != cudaSuccess)\n",
    "{\n",
    "  printf(\"Error: %s\\n\", cudaGetErrorString(err));\n",
    "}\n",
    "```\n",
    "\n",
    "Finalmente, para detectar errores que ocurren de forma asíncrona, por ejemplo, durante la ejecución de un kernel asíncrono, es esencial verificar el estado devuelto por una llamada API de tiempo de ejecución de CUDA de sincronización posterior, como `cudaDeviceSynchronize`, que devolverá un error si uno de los núcleos lanzados anteriormente ha fallado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### CUDA Error Handling Function\n",
    "\n",
    "Puede ser útil crear una macro que envuelva las llamadas a funciones de CUDA para verificar errores. Aquí hay un ejemplo, siéntase libre de usarlo en los ejercicios restantes:\n",
    "\n",
    "```cpp\n",
    "#include <stdio.h>\n",
    "#include <assert.h>\n",
    "\n",
    "inline cudaError_t checkCuda(cudaError_t result)\n",
    "{\n",
    "  if (result != cudaSuccess) {\n",
    "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
    "    assert(result == cudaSuccess);\n",
    "  }\n",
    "  return result;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "\n",
    "/*\n",
    " * The macro can be wrapped around any function returning\n",
    " * a value of type `cudaError_t`.\n",
    " */\n",
    "\n",
    "  checkCuda( cudaDeviceSynchronize() )\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo suma de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#include <sys/time.h>\n",
    "\n",
    "//CUDA\n",
    "#include <cuda.h>\n",
    "\n",
    "double wtime(void)\n",
    "{\n",
    "        static struct timeval   tv0;\n",
    "        double time_;\n",
    "\n",
    "        gettimeofday(&tv0,(struct timezone*)0);\n",
    "        time_=(double)((tv0.tv_usec + (tv0.tv_sec)*1000000));\n",
    "        return( time_/1000000);\n",
    "}\n",
    "\n",
    "\n",
    "void vecAdd(float* A, float* B, float* C,\n",
    "   int n)\n",
    "{\n",
    "\tint i;\n",
    "\tfor (i = 0; i < n; i++)\n",
    "\t\tC[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "\n",
    "__global__ \n",
    "void vecAdd_GPU(float* A, float* B, float* C,\n",
    "   int n)\n",
    "{\n",
    "\tint i;\n",
    "\ti = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "\tif(i<n) \n",
    "\t\tC[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "\tfloat *a, *b, *c, *c_host;\n",
    "\tfloat *a_GPU, *b_GPU, *c_GPU;\n",
    "\n",
    "\tint i, N;\n",
    "\n",
    "\tdouble t0, t1;\n",
    "\n",
    "\n",
    "\tif(argc>1) {\n",
    "\t\tN = atoi(argv[1]); printf(\"N=%i\\n\", N);\n",
    "\t} else {\n",
    "\t\tprintf(\"Error!!!! \\n ./exec number\\n\");\n",
    "\treturn (0);\n",
    "\t}\n",
    "\n",
    "\t// Mallocs CPU\n",
    "\ta  = (float *)malloc(sizeof(float)*N);\n",
    "\tb  = (float *)malloc(sizeof(float)*N);\n",
    "\tc  = (float *)malloc(sizeof(float)*N);\n",
    "\tc_host  = (float *)malloc(sizeof(float)*N);\n",
    "\tfor (i=0; i<N; i++){ a[i] = i-1; b[i] = i;}\n",
    "\n",
    "\t/*****************/\n",
    "\t/* Add Matrix CPU*/\n",
    "\t/*****************/\n",
    "\tt0 = wtime();\n",
    "\tvecAdd(a, b, c, N);\n",
    "\tt1 = wtime(); printf(\"Time CPU=%f\\n\", t1-t0);\n",
    "\n",
    "\t// Get device memory for A, B, C\n",
    "\t// copy A and B to device memory\n",
    "\tcudaMalloc((void **) &a_GPU, N*sizeof(float));\n",
    "\tcudaMemcpy(a_GPU, a, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "\tcudaMalloc((void **) &b_GPU, N*sizeof(float));\n",
    "\tcudaMemcpy(b_GPU, b, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "\tcudaMalloc((void **) &c_GPU, N*sizeof(float));\n",
    "\n",
    "\t// Kernel execution in device\n",
    "\t// (vector add in device)\n",
    "\tdim3 DimBlock(256); // 256 thread per block\n",
    "\tdim3 DimGrid(ceil(N/256.0)+1);\n",
    "\tt0 = wtime();\n",
    "\tvecAdd_GPU<<<DimGrid,DimBlock>>>(a_GPU, b_GPU, c_GPU, N);\n",
    "\tcudaThreadSynchronize();\n",
    "\tt1 = wtime(); printf(\"Time GPU=%f\\n\", t1-t0);\n",
    "\n",
    "\t// copy C to host memory\n",
    "\tcudaMemcpy(c_host, c_GPU, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "\t/************/\n",
    "\t/* Results  */\n",
    "\t/************/\n",
    "\tfor (i=0; i<N; i++)\n",
    "\t\tif(fabs(c[i]-c_host[i])>1e-5){\n",
    "\t\t\tprintf(\"c!=c_host in (%i): \", i);\n",
    "\t\t\tprintf(\"C[%i] = %f C_GPU[%i]=%f\\n\", i, c[i], i, c_host[i] );\n",
    "\t\t}\n",
    "\n",
    "\t/* Free CPU */\n",
    "\tfree(a);\n",
    "\tfree(b);\n",
    "\tfree(c);\n",
    "\tfree(c_host);\n",
    "\n",
    "\tcudaFree(a_GPU); cudaFree(b_GPU); cudaFree(c_GPU);\n",
    "\n",
    "\treturn(1);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compilar con **nvcc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o main main.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./main 1024000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
